# Kullback-Leibler散度

## 熵

Entropy，表示为H，其公式为:
$$
H = -\sum_{i=1}^Np(x_i) \cdot logp(x_i)
$$

* 若使用$log_2$​则熵可以理解为 **编码信息所需的最小比特数**

    ```python
    import math
    
    def cal_entropy_log2(prob_distribution):
    	return -sum(p*math.log2(p) for p in prob_distribution)
    ```



## KL散度

用于衡量两个概率分布之间的差异性的度量方法，其公式为:
$$
D_{KL}(p||q) = \sum_{i=1}^Np(x_i) \cdot (logp(x_i)-logq(x_i)) \\
p为真实概率分布，q为预测概率分布
$$

* 非对称性：p||q 与 q||p 可能不同
* 非负性：KL散度值永远非负。当且仅当两个概率分布完全相同时，KL=0；KL值越大，则说明两个概率分布越不相似
* 非度量性：KL散度并不满足度量空间的性质，尤其是三角不等式

```python
import math

def cal_kl_log2(p,q):
    return sum(p[i] * ( math.log2(p[i]) - math.log2(q[i]) ) )
```

